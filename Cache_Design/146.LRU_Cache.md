# ðŸ”¹ Problem Name: LRU Cache (Design, Medium)

**ðŸ”— Link:**[LeetCode 146 - LRU Cache](https://leetcode.com/problems/lru-cache/)
**ðŸŸ¢ Status:** âœ… Solved

## ðŸ”¹ Quick Breakdown

* **Difficulty Level:** Medium
* **Main Approach:** Use a combination of a **HashMap** (for O(1) lookup) and a **Doubly Linked List** (for O(1) insertion/removal).
* **Core Concept:** Cache design with eviction policy (**Least Recently Used**).
* **When to Use This Pattern?** When you need constant-time cache operations and must evict the least recently accessed item when capacity is full.
* **Mistakes to Avoid:**
  * Forgetting to move the accessed node to the "most recently used" position in `get`.
  * Using a normal list/array (which would cause O(n) for deletions).
  * Not handling existing keys properly in `put` (must remove old node first).

## ðŸ”¹ Optimization Path

* **Brute Force:** Use only a list (but `get` and `put` would be O(n)).
* **Optimized Solution:** Use **HashMap + Doubly Linked List** for O(1) `get` and `put`.
* **Further Improvements Possible?** Not really â€“ this is the optimal data structure design for LRU.

## ðŸ”¹ Solution Breakdown

* **Optimal Approach:** Store nodes in a hashmap for direct access and maintain a doubly linked list to track usage order (most recent near `right`).
* **Why This Works?** Doubly linked list allows O(1) removal and insertion of nodes, and hashmap ensures O(1) access by key.
* **Edge Cases Considered:**
  * Accessing a key not present (return -1).
  * Adding a key that already exists (update its value).
  * Evicting the least recently used element when capacity exceeds.

## ðŸ”¹ Pattern Recognition

* **Belongs To:** Design + Linked List pattern.
* **Common Problems Using This:** LFU Cache, Design Twitter, All O(1) Data Structure.

## ðŸ”¹ Tricks & Patterns Used

* **Doubly Linked List** for constant-time node removal.
* **HashMap** for key-to-node O(1) mapping.
* **Sentinel Nodes** (`left` and `right`) to simplify edge cases.

## ðŸ”¹ Alternative Data Structures

* **Can this problem be optimized using a different DS?** Not significantly â€“ other DS like OrderedDict (Python built-in) internally use a similar structure.
* **Example:** In Python, `collections.OrderedDict` can simplify the implementation.

## ðŸ”¹ Complexity Analysis

* **Time Complexity:**
  * `get` -> O(1)
  * `put` -> O(1)
* **Space Complexity:** O(capacity) due to hashmap + doubly linked list.

## ðŸ”¹ Code Snippet (Optimized)

```python
class Node:
    def __init__(self, key, val):
        self.key, self.val = key, val
        self.prev = self.next = None

class LRUCache:
    def __init__(self, capacity: int):
        self.cap = capacity
        self.cache = {}  # key -> node

        # Left = LRU, Right = MRU
        self.left, self.right = Node(0, 0), Node(0, 0)
        self.left.next, self.right.prev = self.right, self.left

    def remove(self, node):
        prev, nxt = node.prev, node.next
        prev.next, nxt.prev = nxt, prev

    def insert(self, node):
        prev, nxt = self.right.prev, self.right
        prev.next = nxt.prev = node
        node.next, node.prev = nxt, prev

    def get(self, key: int) -> int:
        if key in self.cache:
            self.remove(self.cache[key])
            self.insert(self.cache[key])
            return self.cache[key].val
        return -1

    def put(self, key: int, value: int) -> None:
        if key in self.cache:
            self.remove(self.cache[key])
        self.cache[key] = Node(key, value)
        self.insert(self.cache[key])

        if len(self.cache) > self.cap:
            lru = self.left.next
            self.remove(lru)
            del self.cache[lru.key]
```
